%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage[tone]{tipa}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{ronataswestoldturkic}
\date{May 25, 2023}
\release{2.0}
\author{Viktor Martinović}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{Home}
\label{\detokenize{home:home}}\label{\detokenize{home::doc}}

\section{CLDF dataset derived from ‘West Old Turkic’ by András Róna\sphinxhyphen{}Tas and Árpád Berta from 2011}
\label{\detokenize{home:cldf-dataset-derived-from-west-old-turkic-by-andras-rona-tas-and-arpad-berta-from-2011}}

\subsection{How to cite}
\label{\detokenize{home:how-to-cite}}
\sphinxAtStartPar
If you use these data please cite
\begin{itemize}
\item {} 
\sphinxAtStartPar
the original source: Róna\sphinxhyphen{}Tas, András, \& Berta, Árpád. (2011)
West Old Turkic. Harrassowitz Verlag, Wiesbaden

\item {} 
\sphinxAtStartPar
the derived dataset using the DOI of the \sphinxhref{https://github.com/LoanpyDataHub/ronataswestoldturkic/releases}{particular released version}
you were using

\end{itemize}


\subsection{Description}
\label{\detokenize{home:description}}
\sphinxAtStartPar
This dataset is licensed under a CC\sphinxhyphen{}BY\sphinxhyphen{}4.0 license

\sphinxAtStartPar
Conceptlists in Concepticon:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://concepticon.clld.org/contributions/RonaTas-2011-431}{RonaTas\sphinxhyphen{}2011\sphinxhyphen{}431}

\end{itemize}


\subsection{Notes}
\label{\detokenize{home:notes}}
\sphinxAtStartPar
License: CC BY 4.0\\\\
 Continuous integration: https://dl.circleci.com/status-badge/redirect/gh/LoanpyDataHub/ronataswestoldturkic/tree/main\\\\
 Documentation: https://ronataswestoldturkic.readthedocs.io/en/latest/\\\\

\begin{itemize}
\item {} 
\sphinxAtStartPar
Manually improved alignments with: \sphinxurl{https://digling.org/edictor/}

\item {} 
\sphinxAtStartPar
Created parts of the orthographic profile for Hungarian with:
\sphinxurl{https://digling.org/calc/profiler/}

\item {} 
\sphinxAtStartPar
Skeleton for the Hungarian orthographic profile comes from:
\sphinxurl{https://github.com/dmort27/epitran/blob/master/epitran/data/map/hun-Latn.csv}

\end{itemize}


\subsection{Statistics}
\label{\detokenize{home:statistics}}
\sphinxAtStartPar
CLDF validation: https://github.com/martino-vic/ronataswestoldturkic/actions?query=workflow\%3ACLDF-validation\\\\
 Glottolog: 57\%\\\\
 Concepticon: 62\%\\\\
 Source: 100\%\\\\
BIPA: 100\%\\\\
 CLTS SoundClass: 100\%\\\\

\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Varieties:} 5

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Concepts:} 430

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lexemes:} 1,755

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sources:} 1

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synonymy:} 1.19

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cognacy:} 1,755 cognates in 512 cognate sets (6 singletons)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cognate Diversity:} 0.06

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Invalid lexemes:} 0

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Tokens:} 8,190

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Segments:} 60 (0 BIPA errors, 0 CLTS sound class errors, 60 CLTS
modified)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Inventory size (avg):} 42.80

\end{itemize}


\section{Contributors}
\label{\detokenize{home:contributors}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
GitHub user
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Role
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
András Róna\sphinxhyphen{}Tas
&&
\sphinxAtStartPar
wrote the
entries
beginning with
the letters A,
B, Gy, H, I, J,
K
&
\sphinxAtStartPar
Author
\\
\sphinxhline
\sphinxAtStartPar
† Árpád Berta
&&
\sphinxAtStartPar
draft of the
remaining
letters after K
&
\sphinxAtStartPar
Author
\\
\sphinxhline
\sphinxAtStartPar
László Károly
&&
\sphinxAtStartPar
in charge of
the final
editing work
&
\sphinxAtStartPar
Assistant
\\
\sphinxhline
\sphinxAtStartPar
Viktor
Martinović
&
\sphinxAtStartPar
@martino\sphinxhyphen{}vic
&
\sphinxAtStartPar
CLDF conversion
&
\sphinxAtStartPar
Other
\\
\sphinxhline
\sphinxAtStartPar
Johann\sphinxhyphen{}Mattis
List
&
\sphinxAtStartPar
@LinguList
&
\sphinxAtStartPar
CLDF conversion
&
\sphinxAtStartPar
Other
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{CLDF Datasets}
\label{\detokenize{home:cldf-datasets}}
\sphinxAtStartPar
The following CLDF datasets are available in
\sphinxhref{https://github.com/LoanpyDataHub/ronataswestoldturkic/tree/main/cldf}{cldf}
:
\begin{itemize}
\item {} 
\sphinxAtStartPar
CLDF
\sphinxhref{https://github.com/cldf/cldf/tree/master/modules/Wordlist}{Wordlist}
at \sphinxhref{https://github.com/LoanpyDataHub/ronataswestoldturkic/blob/main/cldf/cldf-metadata.json}{cldf/cldf\sphinxhyphen{}metadata.json}

\end{itemize}

\sphinxstepscope


\chapter{TL;DR}
\label{\detokenize{TL;DR:id1}}\label{\detokenize{TL;DR::doc}}\phantomsection\label{\detokenize{TL;DR:module-ronataswestoldturkiccommands.__init__}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.\_\_init\_\_@\spxentry{ronataswestoldturkiccommands.\_\_init\_\_}}\index{ronataswestoldturkiccommands.\_\_init\_\_@\spxentry{ronataswestoldturkiccommands.\_\_init\_\_}!module@\spxentry{module}}
\sphinxAtStartPar
Open terminal in folder containing GitHub projects or
in a newly created folder and run the commands below to…

\sphinxAtStartPar
…create the contents of the cldf\sphinxhyphen{}folder (download size: 1GB+):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv\PYG{+w}{ }\PYG{o}{\PYGZam{}\PYGZam{}}\PYG{+w}{ }\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate

git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/martino\PYGZhy{}vic/ronataswestoldturkic.git
mkdir\PYG{+w}{ }concepticon
\PYG{n+nb}{cd}\PYG{+w}{ }concepticon
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/concepticon/concepticon\PYGZhy{}data.git
\PYG{n+nb}{cd}\PYG{+w}{ }..
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/glottolog/glottolog.git
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/cldf\PYGZhy{}clts/clts.git

pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}e\PYG{+w}{ }ronataswestoldturkic
pip\PYG{+w}{ }install\PYG{+w}{ }loanpy
pip\PYG{+w}{ }install\PYG{+w}{ }pytest\PYGZhy{}cldf

\PYG{n+nb}{cd}\PYG{+w}{ }ronataswestoldturkic
cldfbench\PYG{+w}{ }lexibank.makecldf\PYG{+w}{ }lexibank\PYGZus{}ronataswestoldturkic.py\PYG{+w}{  }\PYGZhy{}\PYGZhy{}concepticon\PYGZhy{}version\PYG{o}{=}v3.0.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYGZhy{}version\PYG{o}{=}v4.5\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYGZhy{}version\PYG{o}{=}v2.2.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}concepticon\PYG{o}{=}../concepticon/concepticon\PYGZhy{}data\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYG{o}{=}../glottolog\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYG{o}{=}../clts

pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cldf\PYGZhy{}metadata\PYG{o}{=}cldf/cldf\PYGZhy{}metadata.json\PYG{+w}{ }test.py
\end{sphinxVerbatim}

\sphinxAtStartPar
…create the contents of the edictor\sphinxhyphen{}folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.maketoedict\PYGZus{}rc\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.maketoedict\PYGZus{}ad\PYG{+w}{ }WOT\PYG{+w}{ }EAH

cldfbench\PYG{+w}{ }ronataswestoldturkic.cvgapedicted\PYG{+w}{ }WOT\PYG{+w}{ }EAH

cldfbench\PYG{+w}{ }ronataswestoldturkic.evaledicted\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.evaledicted\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
…create the contents of the loanpy\sphinxhyphen{}folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.mineEAHinvs\PYG{+w}{ }invs.json
cldfbench\PYG{+w}{ }ronataswestoldturkic.makeheur\PYG{+w}{ }EAH\PYG{+w}{ }heur.json
cldfbench\PYG{+w}{ }ronataswestoldturkic.minesc\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.minesc\PYG{+w}{ }WOT\PYG{+w}{ }EAH\PYG{+w}{ }heur.json
cldfbench\PYG{+w}{ }ronataswestoldturkic.vizsc\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.vizsc\PYG{+w}{ }WOT\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.evalsc\PYG{+w}{ }H\PYG{+w}{ }EAH\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}[10, 100, 500, 700, 1000, 5000, 7000]\PYGZdq{}}
cldfbench\PYG{+w}{ }ronataswestoldturkic.evalsc\PYG{+w}{ }WOT\PYG{+w}{ }EAH\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}[10, 100, 500, 700, 1000, 5000, 7000]\PYGZdq{}}\PYG{+w}{ }True\PYG{+w}{ }True\PYG{+w}{ }heur.json
cldfbench\PYG{+w}{ }ronataswestoldturkic.plot\PYGZus{}eval\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.plot\PYGZus{}eval\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Part 1: Create CLDF}
\label{\detokenize{mkcldf:part-1-create-cldf}}\label{\detokenize{mkcldf::doc}}
\sphinxAtStartPar
The following six steps will guide you through the process of
converting raw language data to CLDF. Each step can be found in the
\sphinxhref{https://app.circleci.com/pipelines/github/LoanpyDataHub/ronataswestoldturkic}{continuous integration workflow}
as well. The data we are converting comes from the etymological dictionary
\sphinxhref{https://www.goodreads.com/book/show/13577601-west-old-turkic}{“West Old Turkic”}
(Róna\sphinxhyphen{}Tas and Berta 2011),
which contains modern Hungarian words as headwords, together with their
documented and reconstructed ancestor forms, including their
West Old Turkic donor words. West Old Turkic, also called \sphinxhref{https://glottolog.org/resource/languoid/id/bolg1249}{Proto\sphinxhyphen{}Bolgar}, is the parent
language of the western branch
of Turkic languages. The raw data contains only a small fraction of the
contents of the dictionary. If you are passionate about pdf\sphinxhyphen{}wrangling,
Mongolic, Turkic, or Finno\sphinxhyphen{}Ugric languages and want to expand this data set,
definitely check out the \sphinxhref{https://github.com/martino-vic/ronataswestoldturkic/blob/main/CONTRIBUTING.md}{contribution guidelines}
and let’s get in touch!


\section{Step 1: Clone the repository}
\label{\detokenize{mkcldf:step-1-clone-the-repository}}
\sphinxAtStartPar
It is strongly recommended to create and activate a virtual environment first.
This can be for example done by running

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3\PYG{+w}{ }\PYGZhy{}m\PYG{+w}{ }venv\PYG{+w}{ }venv
\PYG{n+nb}{source}\PYG{+w}{ }venv/bin/activate
\end{sphinxVerbatim}

\sphinxAtStartPar
To deactivate it, run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
deactivate
\end{sphinxVerbatim}

\sphinxAtStartPar
in an activated virtual environment. And to delete it, deactivate it and run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rm\PYG{+w}{ }\PYGZhy{}r\PYG{+w}{ }venv
\end{sphinxVerbatim}

\sphinxAtStartPar
Once You have set it up and activated it, you can start the process
of CLDF conversion by installing this repository:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/martino\PYGZhy{}vic/ronataswestoldturkic.git
\end{sphinxVerbatim}

\sphinxAtStartPar
Originally, the skeleton of the repository was created using this command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }new
\end{sphinxVerbatim}

\sphinxAtStartPar
and answering the follow\sphinxhyphen{}up questions. More on this can be read in the
\sphinxhref{https://github.com/cldf/cldfbench/blob/master/doc/tutorial.md}{cldfbench tutorial}.


\section{Step 2: Clone reference catalogues}
\label{\detokenize{mkcldf:step-2-clone-reference-catalogues}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://glottolog.org/}{Glottolog} (Hammarström et al. 2022)
to reference the languages in the repo.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://concepticon.clld.org/}{Concepticon} (List et al. 2023) to
reference concepts.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://clts.clld.org/}{CLTS} (List et al. 2021) to reference IPA
characters

\end{itemize}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
The following repositories will take over 1GB in disk\sphinxhyphen{}space. If you skip
cloning them,
add a \sphinxcode{\sphinxupquote{\sphinxhyphen{}\sphinxhyphen{}dev}} flag to the command running the lexibank script in step 5.
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mkdir\PYG{+w}{ }concepticon
\PYG{n+nb}{cd}\PYG{+w}{ }concepticon
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/concepticon/concepticon\PYGZhy{}data.git
\PYG{n+nb}{cd}\PYG{+w}{ }..
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/glottolog/glottolog.git
git\PYG{+w}{ }clone\PYG{+w}{ }https://github.com/cldf\PYGZhy{}clts/clts.git
\end{sphinxVerbatim}


\section{Step 3: Install commands and loanpy}
\label{\detokenize{mkcldf:step-3-install-commands-and-loanpy}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{\sphinxhyphen{}e}} flag will install all necessary dependencies in development mode.
I.e. if you modify any code in those repositories, changes will apply
immediately. Remember to run this command from the directory where
all your GitHub repositories are stored.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }\PYGZhy{}e\PYG{+w}{ }ronataswestoldturkic
\end{sphinxVerbatim}

\sphinxAtStartPar
Install \sphinxhref{https://loanpy.readthedocs.io/en/latest/home.html}{loanpy}
(Martinović 2023):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }loanpy
\end{sphinxVerbatim}


\section{Step 4: Create IPA transcriptions}
\label{\detokenize{mkcldf:step-4-create-ipa-transcriptions}}
\sphinxAtStartPar
If the folder \sphinxcode{\sphinxupquote{etc}} contains a folder \sphinxcode{\sphinxupquote{orthography}} with files having the
same
name as the language IDs in \sphinxcode{\sphinxupquote{etc/languages.tsv}}, CLDF automatically creates
new columns in \sphinxcode{\sphinxupquote{cldf/forms.csv}} that contain
IPA transcriptions and segmentations. In this step, we are creating the
IPA transcription rules for our raw data. The data in the original source
is \sphinxhref{https://en.wikipedia.org/wiki/Uralic\_Phonetic\_Alphabet}{UPA}
\sphinxhyphen{}transcribed, which is here converted to \sphinxhref{https://www.internationalphoneticassociation.org/sites/default/files/IPA\_Kiel\_2015.pdf}{IPA}.
Hungarian
headwords in the original source were provided in both, their official
orthographic form and their transcription to the internal
phonetic alphabet. We included only the modern orthographic forms.
Therefore, an additional orthography had to be added for IPA\sphinxhyphen{}transcription.
The skeleton for this comes from a \sphinxhref{https://github.com/dmort27/epitran/blob/master/epitran/data/map/hun-Latn.csv}{file in the open source library epitran}.
Some of the combinatorics for the hard\sphinxhyphen{}coded transcription rules were created
with an online tool called \sphinxhref{https://digling.org/calc/profiler/}{Orthographic Profiler}. The transcription profiles were
written to the files \sphinxcode{\sphinxupquote{H.tsv}} \sphinxcode{\sphinxupquote{EAH.tsv}} \sphinxcode{\sphinxupquote{LAH.tsv}} \sphinxcode{\sphinxupquote{OH.tsv}} and
\sphinxcode{\sphinxupquote{WOT.tsv}} in the folder \sphinxcode{\sphinxupquote{etc/orthography}}.


\section{Step 5: Run lexibank script}
\label{\detokenize{mkcldf:step-5-run-lexibank-script}}
\sphinxAtStartPar
This script combines files from the raw and etc folders and creates and
populates the folder cldf

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd}\PYG{+w}{ }ronataswestoldturkic
cldfbench\PYG{+w}{ }lexibank.makecldf\PYG{+w}{ }lexibank\PYGZus{}ronataswestoldturkic.py\PYG{+w}{  }\PYGZhy{}\PYGZhy{}concepticon\PYGZhy{}version\PYG{o}{=}v3.0.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYGZhy{}version\PYG{o}{=}v4.5\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYGZhy{}version\PYG{o}{=}v2.2.0\PYG{+w}{ }\PYGZhy{}\PYGZhy{}concepticon\PYG{o}{=}../concepticon/concepticon\PYGZhy{}data\PYG{+w}{ }\PYGZhy{}\PYGZhy{}glottolog\PYG{o}{=}../glottolog\PYG{+w}{ }\PYGZhy{}\PYGZhy{}clts\PYG{o}{=}../clts
\end{sphinxVerbatim}

\sphinxAtStartPar
Below is a detailed description of what the script does. See also the
tutorial at \sphinxurl{https://calc.hypotheses.org/3318}, which has many similarities.
This is the first lexibank script that uses the
\sphinxcode{\sphinxupquote{args.writer.align\_cognates()}} prompt for automatic cognate alignment
(see \sphinxhref{https://github.com/lexibank/pylexibank/issues/267\#issuecomment-1418959540}{discussion on GitHub}).
It is also one of the first CLDF repositories to link various historical
stages of the same language to each other, covering horizontal and vertical
transfers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{ast}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pathlib}

\PYG{k+kn}{import} \PYG{n+nn}{attr}
\PYG{k+kn}{from} \PYG{n+nn}{clldutils}\PYG{n+nn}{.}\PYG{n+nn}{misc} \PYG{k+kn}{import} \PYG{n}{slug}
\PYG{k+kn}{from} \PYG{n+nn}{loanpy}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{IPA}
\PYG{k+kn}{from} \PYG{n+nn}{pylexibank} \PYG{k+kn}{import} \PYG{n}{Dataset} \PYG{k}{as} \PYG{n}{BaseDataset}\PYG{p}{,} \PYG{n}{FormSpec}\PYG{p}{,} \PYG{n}{Lexeme}
\end{sphinxVerbatim}

\sphinxAtStartPar
First, we import three inbuilt Python\sphinxhyphen{}libraries.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The \sphinxhref{https://docs.python.org/3/library/ast.html}{ast} library will turn
the strings “True” and “False” into booleans.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://docs.python.org/3/library/json.html}{json} library
will be used to read the data\sphinxhyphen{}cleaning instructions for the
\sphinxcode{\sphinxupquote{pylexibank.FormSpec}} class.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://docs.python.org/3/library/pathlib.html}{pathlib} library
will be used to define file paths

\end{itemize}

\sphinxAtStartPar
Then, we import functionalities from various third\sphinxhyphen{}party libraries.
These were installed when running
\sphinxcode{\sphinxupquote{pip install \sphinxhyphen{}e ronataswestoldturkic}} eariler.
\begin{itemize}
\item {} 
\sphinxAtStartPar
With the \sphinxhref{https://www.attrs.org/en/stable/index.html}{attrs} library
we will create the custom language class with custom columns in the output
file \sphinxcode{\sphinxupquote{cldf/forms.csv}}.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://clldutils.readthedocs.io/en/latest/misc.html\#clldutils.misc.slug}{slug}
function from the \sphinxhref{https://github.com/clld/clldutils}{clldutils} library
will be used to format some IDs.

\item {} 
\sphinxAtStartPar
The \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.IPA}{IPA}
class from the \sphinxhref{https://loanpy.readthedocs.io/en/latest/home.html}{loanpy}
library will be used to create the phonotactic structures of words.

\item {} 
\sphinxAtStartPar
The classes from the \sphinxhref{https://pypi.org/project/pylexibank/}{pylexibank}
library are all related to specifying the output format. \sphinxcode{\sphinxupquote{Dataset}} for
example loads the default data format, \sphinxcode{\sphinxupquote{Lexeme}} will be used to customise
it, and \sphinxcode{\sphinxupquote{FormSpec}} will be used to document the cleaning of the raw data.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ipa} \PYG{o}{=} \PYG{n}{IPA}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here, we are creating an instance of loanpy’s IPA class which loads
a list of 1464 IPA sounds that are vowels into its \sphinxcode{\sphinxupquote{.vowels}} attribute.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{etc/formspec.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{REP} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k}\PYG{p}{,} \PYG{n}{v} \PYG{o+ow}{in} \PYG{n}{json}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
The variable REP stands for ‘replacements’ and will later be used to create
the column “forms” from the column “values”, where replacements are
hard\sphinxhyphen{}coded. Since the number of transformations is too large to include them
in this script, they were written to a json\sphinxhyphen{}file, which is loaded here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@attr}\PYG{o}{.}\PYG{n}{s}
\PYG{k}{class} \PYG{n+nc}{CustomLexeme}\PYG{p}{(}\PYG{n}{Lexeme}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{CV\PYGZus{}Segments} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{n}{ProsodicStructure} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{n}{FB\PYGZus{}VowelHarmony} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{n}{Year} \PYG{o}{=} \PYG{n}{attr}\PYG{o}{.}\PYG{n}{ib}\PYG{p}{(}\PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we define custom columns that are not included by default, using
\sphinxhref{https://www.attrs.org/en/stable/api-attr.html\#attr.ib}{attr.ib} and the
Lexeme class that we have imported earlier.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{has\PYGZus{}harmony}\PYG{p}{(}\PYG{n}{segments}\PYG{p}{)}\PYG{p}{:}
\PYG{k}{if} \PYG{n+nb}{any}\PYG{p}{(}\PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{segments} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y\textipa{:}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ø}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ø\textipa{:}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n+nb}{any}\PYG{p}{(}\PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{segments} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a\textipa{:}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\textipa{6}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\textipa{W}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u\textipa{:}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{k+kc}{False}
\PYG{k}{return} \PYG{k+kc}{True}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we define a function that checks whether a word has vowel harmony or not.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}loan}\PYG{p}{(}\PYG{n}{loan}\PYG{p}{,} \PYG{n}{language}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{ast}\PYG{o}{.}\PYG{n}{literal\PYGZus{}eval}\PYG{p}{(}\PYG{n}{loan}\PYG{p}{)} \PYG{k}{if} \PYG{n}{language} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{WOT}\PYG{l+s+s2}{\PYGZdq{}} \PYG{k}{else} \PYG{k+kc}{True}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we convert the textual information from the column \sphinxcode{\sphinxupquote{WOT\_loan}} in
\sphinxcode{\sphinxupquote{raw/wot.tsv}} to booleans with \sphinxhref{https://docs.python.org/3/library/ast.html\#ast.literal\_eval}{literal\_eval}.
This has to be a separate function and can’t be implemented through a lambda.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Dataset}\PYG{p}{(}\PYG{n}{BaseDataset}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{dir} \PYG{o}{=} \PYG{n}{pathlib}\PYG{o}{.}\PYG{n}{Path}\PYG{p}{(}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{parent}
    \PYG{n+nb}{id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ronataswestoldturkic}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{lexeme\PYGZus{}class} \PYG{o}{=} \PYG{n}{CustomLexeme}
    \PYG{n}{form\PYGZus{}spec} \PYG{o}{=} \PYG{n}{FormSpec}\PYG{p}{(}\PYG{n}{separators}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{first\PYGZus{}form\PYGZus{}only}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                         \PYG{n}{replacements}\PYG{o}{=} \PYG{n}{REP}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we define a class and inherit the default format \sphinxcode{\sphinxupquote{BaseDataset}} that we
have imported in the beginning. \sphinxcode{\sphinxupquote{dir}} is the working directory and is
defined with the help of \sphinxcode{\sphinxupquote{pathlib}} that we have imported in the beginning.
\sphinxcode{\sphinxupquote{id}} is the name of the repository. In \sphinxcode{\sphinxupquote{lexeme\_class}} we are plugging in
the custom columns that we have created earlier. In \sphinxcode{\sphinxupquote{form\_spec}} we are
plugging in the data\sphinxhyphen{}cleaning rules that were hard coded in
\sphinxcode{\sphinxupquote{etc/formspec.json}} and read into the \sphinxcode{\sphinxupquote{REP}} variable earlier, using the
\sphinxcode{\sphinxupquote{FormSpec}} class we have imported in the beginning.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cmd\PYGZus{}makecldf}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{args}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
This function is being run when summoning the lexibank script from the
command line. It converts the data from the folders \sphinxcode{\sphinxupquote{raw}} and \sphinxcode{\sphinxupquote{etc}} to
standardised CLDF data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{cldf}\PYG{o}{.}\PYG{n}{add\PYGZus{}component}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BorrowingTable}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are creating a \sphinxhref{https://cldf.clld.org/v1.0/terms.rdf\#BorrowingTable}{BorrowingTable}
\sphinxcode{\sphinxupquote{cldf/borrowings.csv}} which will contain the IDs of donor and recipient
words.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}sources}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added sources}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In the first line we are adding the bibliography from \sphinxcode{\sphinxupquote{raw/sources.bib}}.
This is a \sphinxhref{https://en.wikipedia.org/wiki/BibTeX}{BibTex} file containing
references to all sources from which the data in the folders \sphinxcode{\sphinxupquote{raw}} and
\sphinxcode{\sphinxupquote{etc}} was acquired. In the second line we log to the console
that the sources were added successfully. This can be helpful for debugging.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{concepts} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{concept} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{concepts}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{idx} \PYG{o}{=} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n}{slug}\PYG{p}{(}\PYG{n}{concept}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ENGLISH}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{concepts}\PYG{p}{[}\PYG{n}{concept}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ENGLISH}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{idx}
    \PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}concept}\PYG{p}{(}
            \PYG{n}{ID}\PYG{o}{=}\PYG{n}{idx}\PYG{p}{,}
            \PYG{n}{Name}\PYG{o}{=}\PYG{n}{concept}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ENGLISH}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Concepticon\PYGZus{}ID}\PYG{o}{=}\PYG{n}{concept}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CONCEPTICON\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{Concepticon\PYGZus{}Gloss}\PYG{o}{=}\PYG{n}{concept}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CONCEPTICON\PYGZus{}GLOSS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{p}{)}

\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added concepts}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This section of the script creates the file \sphinxcode{\sphinxupquote{cldf/parameters.csv}}, which
links the translations of words to concepts in
\sphinxhref{https://concepticon.clld.org/}{Concepticon}. It is based on
\sphinxcode{\sphinxupquote{etc/concepts.tsv}}, which was created through multiple steps. At first, the
translations were mapped automatically with the
\sphinxhref{https://pypi.org/project/pysem/}{pysem} library. Then, these mappings were
manually refined and requested to be submitted to Concepticon through a
\sphinxhref{https://github.com/concepticon/concepticon-data/pull/1240}{Pull Request on GitHub}. After some
discussion and further refinement, the conceptlist was submitted and is
available
\sphinxhref{https://concepticon.clld.org/contributions/RonaTas-2011-431}{here}.
The file \sphinxcode{\sphinxupquote{etc/concepts.tsv}} was then accordingly copied again from
\sphinxhref{https://github.com/concepticon/concepticon-data/blob/master/concepticondata/conceptlists/RonaTas-2011-431.tsv}{GitHub}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{comments} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{etc\PYGZus{}dir}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{comments.tsv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{p}{)}
\PYG{n}{comments} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{line}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:} \PYG{n}{line}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{k}{for} \PYG{n}{line} \PYG{o+ow}{in} \PYG{n}{comments}\PYG{p}{\PYGZcb{}}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added comments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are reading the file \sphinxcode{\sphinxupquote{etc/comments.tsv}}, which was originally
created with a custom script from an additional column in \sphinxcode{\sphinxupquote{raw/wot.tsv}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{languages} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}languages}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{added languages}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here, we read the file \sphinxcode{\sphinxupquote{etc/languages.tsv}} which contains the references to
\sphinxhref{https://glottolog.org/}{Glottolog} and write the information to
\sphinxcode{\sphinxupquote{cldf/languages.csv}}. Out of the five languages in this repository, only
Hungarian is clearly present in Glottolog. Old Hungarian is missing, but a
\sphinxhref{https://github.com/glottolog/glottolog/issues/899}{request} was opened to
add it and after some discussion there seems to be a plan to include it in a
future version of Glottolog. Early and Late Ancient Hungarian are categories
that are only used in our source and can therefore not be added to Glottolog,
according to its \sphinxhref{https://glottolog.org/glottolog/glottologinformation\#principles}{principles}. Even
though Glottolog does not contain Proto\sphinxhyphen{}languages, the nodes in their
language
family trees have their own glotto\sphinxhyphen{}codes, which serve as a reasonable proxy.
In the case of West Old Turkic, the ancestor language of Bolgarian and Chuvash,
we can thus insert the glotto\sphinxhyphen{}code of \sphinxhref{https://glottolog.org/resource/languoid/id/bolg1249}{Bolgar} into our table.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{raw\PYGZus{}dir}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wot.tsv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{p}{)}
\PYG{n}{header} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{cognates} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{cogidx} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{borrid} \PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we read the file \sphinxcode{\sphinxupquote{raw/wot.tsv}} and define some variables that we are
going to use in a bit.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{cognates} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{header}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{concept} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{]}
    \PYG{n}{eah} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we will loop through the raw data \sphinxcode{\sphinxupquote{raw/wot.tsv}} row by row from top to
bottom and define some variables that we will need later. The column “ENGLISH”
is hard\sphinxhyphen{}coded as column seven. If it was to be moved to a different index for
which ever reason, the index in this part of the code would need to be updated
accordingly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{language} \PYG{o+ow}{in} \PYG{n}{languages}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we loop from left to right through the columns of each row, which contain
data relating to words in different languages. The languages themselves were
defined earlier in \sphinxcode{\sphinxupquote{etc/languages.tsv}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cog} \PYG{o}{=} \PYG{n}{cognates}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{language}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we are reading the specific word in the specific language from the raw
data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{concept} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{cognates}\PYG{p}{:}
    \PYG{n}{cognates}\PYG{p}{[}\PYG{n}{concept}\PYG{p}{]} \PYG{o}{=} \PYG{n}{cogidx}
    \PYG{n}{cogidx} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{cogid} \PYG{o}{=} \PYG{n}{cognates}\PYG{p}{[}\PYG{n}{concept}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
The goal of this section is simply to assign a unique cognate ID to each
English translation in column seven. Identical translations will get identical
IDs. This value will appear in the column \sphinxcode{\sphinxupquote{Cognacy}} in the output file
\sphinxcode{\sphinxupquote{cldf/forms.csv}} later.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{lex} \PYG{o+ow}{in} \PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}forms\PYGZus{}from\PYGZus{}value}\PYG{p}{(}
        \PYG{n}{Language\PYGZus{}ID}\PYG{o}{=}\PYG{n}{language}\PYG{p}{,}
        \PYG{n}{Parameter\PYGZus{}ID}\PYG{o}{=}\PYG{n}{concepts}\PYG{p}{[}\PYG{n}{concept}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{Value}\PYG{o}{=}\PYG{n}{cog}\PYG{p}{,}
        \PYG{n}{Comment}\PYG{o}{=}\PYG{n}{comments}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{concept}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{Source}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wot}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{Loan}\PYG{o}{=}\PYG{n}{get\PYGZus{}loan}\PYG{p}{(}\PYG{n}{cognates}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{WOT\PYGZus{}loan}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{language}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{Cognacy}\PYG{o}{=}\PYG{n}{cogid}\PYG{p}{,}
        \PYG{n}{Year}\PYG{o}{=}\PYG{n}{cognates}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Year}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}

\sphinxAtStartPar
This is arguably the most important part of the script. It creates the file
\sphinxcode{\sphinxupquote{cldf/forms.csv}} which will serve as the main input file for further
analyses. \sphinxcode{\sphinxupquote{args.writer.add\_forms\_from\_value}} creates the file, through which
we then loop. The arguments in the brackets are the column names.
\sphinxcode{\sphinxupquote{Language\_ID}} is the name of the language according to
\sphinxcode{\sphinxupquote{etc/languages.tsv}}. \sphinxcode{\sphinxupquote{Parameter\_ID}} references the relevant row in
\sphinxcode{\sphinxupquote{parameters.csv}}, which was created in an earlier code\sphinxhyphen{}block. \sphinxcode{\sphinxupquote{Value}} is
the original raw data. The column \sphinxcode{\sphinxupquote{Form}} is automatically being created from
column \sphinxcode{\sphinxupquote{Value}} by applying the cleaning procedure specified in
\sphinxcode{\sphinxupquote{etc/formspec.json}} which was read into the
\sphinxcode{\sphinxupquote{REP}} variable in the beginning. The column
\sphinxcode{\sphinxupquote{Comment}} uses the English translations as dictionary keys to look up the
according comment as specified in \sphinxcode{\sphinxupquote{etc/comments.tsv}}. The entire data set is
based on one source. In the column \sphinxcode{\sphinxupquote{Source}} we are specifying the BibTex key
of it, as described in \sphinxcode{\sphinxupquote{raw/sources.bib}}. The column \sphinxcode{\sphinxupquote{Loan}} specifies
whether a word is a loanword or not. This information is stored in column
\sphinxcode{\sphinxupquote{WOT\_loan}} in \sphinxcode{\sphinxupquote{raw/wot.tsv}} and is converted to a boolean through the
function \sphinxcode{\sphinxupquote{get\_loan}} which was described in an earlier code\sphinxhyphen{}block.
\sphinxcode{\sphinxupquote{Cognacy}} assigns a unique identifier to each cognate set in the form of an
integer that starts at 1 and is incremented by 1 with each new cognate set.
The column \sphinxcode{\sphinxupquote{Year}} is another custom column that was specified in the
\sphinxcode{\sphinxupquote{CustomLexeme}} class earlier. This information is provided in column
\sphinxcode{\sphinxupquote{Year}} in \sphinxcode{\sphinxupquote{raw/wot.tsv}} and represents each word’s year of first
appearance in a written source.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CV\PYGZus{}Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ipa}\PYG{o}{.}\PYG{n}{get\PYGZus{}clusters}\PYG{p}{(}\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ProsodicStructure}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ipa}\PYG{o}{.}\PYG{n}{get\PYGZus{}prosody}\PYG{p}{(}
                                       \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
                                       \PYG{p}{)}
\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FB\PYGZus{}VowelHarmony}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{has\PYGZus{}harmony}\PYG{p}{(}\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Segments}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we populate three more columns which take information from the columns of
the newly generated \sphinxcode{\sphinxupquote{cldf/forms.csv}} as input. That’s why they have to be
populated through a loop rather than in the brackets of the earlier function.
The column \sphinxcode{\sphinxupquote{CV\_Segments}} takes the column \sphinxcode{\sphinxupquote{Segments}} of \sphinxcode{\sphinxupquote{cldf/forms.csv}}
as input, which in turn is automatically generated from the information stored
in \sphinxcode{\sphinxupquote{etc/orthography}}. These columns are based on tokenised IPA\sphinxhyphen{}strings,
that were read from the files in \sphinxcode{\sphinxupquote{etc/orthography}}. The column
\sphinxcode{\sphinxupquote{ProsodicStructure}} is created with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.IPA.get\_prosody}{loanpy.utils.IPA.get\_prosody}.
The column \sphinxcode{\sphinxupquote{FB\_VowelHarmony}} checks if a word has front\sphinxhyphen{}back vowel harmony
based on a function defined earlier in this script.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{language} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{EAH}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{eah} \PYG{o}{=} \PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
This line is storing the ID of the relevant word in \sphinxcode{\sphinxupquote{cldf/forms.csv}}, so it
can later be referenced in \sphinxcode{\sphinxupquote{cldf/borrowings.csv}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}cognate}\PYG{p}{(}
        \PYG{n}{lexeme}\PYG{o}{=}\PYG{n}{lex}\PYG{p}{,}
        \PYG{n}{Cognateset\PYGZus{}ID}\PYG{o}{=}\PYG{n}{cogid}\PYG{p}{,}
        \PYG{n}{Source}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wot}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we create the table \sphinxcode{\sphinxupquote{cldf/cognates.csv}}. This is the table where
automated alignments will be carried out, which can be used for further
analyses. The term \sphinxcode{\sphinxupquote{cognate}} here is used in its broader sense and includes
all words that go back to the same etymon.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{if} \PYG{n}{language} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{WOT}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{and} \PYG{n}{eah}\PYG{p}{:}
        \PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{objects}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BorrowingTable}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{borrid}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Parameter\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Target\PYGZus{}Form\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{eah}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Source\PYGZus{}Form\PYGZus{}ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ID}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{lex}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Source}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
            \PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{borrid} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{eah} \PYG{o}{=} \PYG{k+kc}{None}

\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FormTable, CognateTable, BorrowingTable: done}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here the file \sphinxcode{\sphinxupquote{cldf/borrowings.csv}} is created. It contains reference keys
to \sphinxcode{\sphinxupquote{cldf/forms.csv}} to identify each donor and recipient word. It makes sure
that only those concepts are included where a form in both West Old Turkic
(the donor language) and Early Ancient Hungarian (the recipient language)
exist. In the end, we print the information to the logger, that the three
tables were created successfully from the current loop.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{args}\PYG{o}{.}\PYG{n}{writer}\PYG{o}{.}\PYG{n}{align\PYGZus{}cognates}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{args}\PYG{o}{.}\PYG{n}{log}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cognate alignment: done}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This is the final line, which creates automated alignments with the
\sphinxhref{https://lingpy.org/}{lingpy} library. They are added to a newly created
column called \sphinxcode{\sphinxupquote{ALIGNMENTS}} in \sphinxcode{\sphinxupquote{etc/cognates.csv}}. This repository is the
first use\sphinxhyphen{}case for this functionality (see \sphinxhref{https://github.com/lexibank/pylexibank/issues/267\#issuecomment-1418959540}{discussion on GitHub}
).

\sphinxAtStartPar
This is how your console should approximately look after the conversion:

\noindent\sphinxincludegraphics{{consoleoutput}.png}


\section{Step 6: Test with pytest\sphinxhyphen{}cldf whether the dataset is CLDF\sphinxhyphen{}conform}
\label{\detokenize{mkcldf:step-6-test-with-pytest-cldf-whether-the-dataset-is-cldf-conform}}
\sphinxAtStartPar
Now that the conversion has run successfully, the only thing left to do is to
verify that the data conforms to the CLDF standard:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip\PYG{+w}{ }install\PYG{+w}{ }pytest\PYGZhy{}cldf
pytest\PYG{+w}{ }\PYGZhy{}\PYGZhy{}cldf\PYGZhy{}metadata\PYG{o}{=}cldf/cldf\PYGZhy{}metadata.json\PYG{+w}{ }test.py
\end{sphinxVerbatim}

\sphinxAtStartPar
This will run one single test with the
\sphinxhref{https://docs.pytest.org/en/7.2.x/}{pytest} library, which should pass.
And with this we have converted our raw data to CLDF and thus finished part
one. Click on the \sphinxcode{\sphinxupquote{Next}}\sphinxhyphen{}button to get to part two.

\sphinxstepscope


\chapter{Part 2: Align words with Edictor}
\label{\detokenize{mkedictor:part-2-align-words-with-edictor}}\label{\detokenize{mkedictor::doc}}
\sphinxAtStartPar
In this section we are going to further process the CLDF data by adding
alignments to related words. Alignments are there to tell you which sound
corresponds to which in an etymology. For example, in “a\textipa{:}ko\textipa{S} \textless{} a\textipa{:}\textipa{g}o\textipa{S}” the
sound correspondences are “a\textipa{:} \textless{} a\textipa{:}, k \textless{} g, o \textless{} o, \textipa{S} \textless{} \textipa{S}”. When we align words
between a descendant and an ancestor language like Hungarian and Old
Hungarian, words are transferred through time and sound correspondences are
called historical sound changes. When we align recipient and donor languages
like Early Ancient Hungarian and West Old Turkic, words are transferred
through space and sound correspondences are called sound adaptations.
These two types of transfers lead to fairly regular transformations.
Nevertheless they differ in many ways from each other, both in how they are
conceptualised and researched, as well as in their outcomes. Having good
alignments for each individual etymology is the basis for identifying
larger correspondence patterns over a data set. There is a multitude of
automatic alignment\sphinxhyphen{}algorithms out there, most of which are inspired by
bioinformatics. However, none of them is good enough yet to replace
expert\sphinxhyphen{}knowledge. Therefore, a hybrid approach is currently the best:
At first, related words are automatically aligned. This is done with the
\sphinxhref{https://lingpy.org/}{lingpy} package, which contains various different
alignment methods, and the \sphinxhref{https://pypi.org/project/loanpy/}{loanpy}
package, which contains a custom alignment method specifically designed
for the type of data of our use case. Then, they are uploaded to the
\sphinxhref{https://digling.org/edictor/}{Edictor}, an interactive tool to manually
align words. Once the results of the algorithmic approach are complemented
by expert knkowledge, the data can be downloaded. Finally, the format of
the manually edited data can be checked for suitability as input for
loanpy.

\sphinxAtStartPar
The following three steps will guide you through the process of converting
data to Edictor\sphinxhyphen{}suitable input, uploading them to Edictor, editing them there,
downloading them, post\sphinxhyphen{}editing them locally, and evaluating whether their
format is suitable as input for loanpy.


\begin{sphinxseealso}{See also:}

\sphinxAtStartPar
\sphinxhref{https://digling.org/edictor/}{Edictor}

\sphinxAtStartPar
\sphinxhref{https://pypi.org/project/loanpy/}{LoanPy}


\end{sphinxseealso}



\section{Step 1: Create input files for Edictor}
\label{\detokenize{mkedictor:step-1-create-input-files-for-edictor}}
\sphinxAtStartPar
During the CLDF\sphinxhyphen{}conversion we have added automated alignments to the column
\sphinxcode{\sphinxupquote{ALIGNMENTS}} in \sphinxcode{\sphinxupquote{cldf/cognates.csv}}. But these are the results of aligning
cognates from \sphinxstyleemphasis{all} available languages to each other, while for our own
analyses we need alignments for only two languages. Therefore, we
first create the input data for Edictor for sound corrrespondences between
Hungarian and Early Ancient Hungarian words with following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.maketoedict\PYGZus{}rc\PYG{+w}{ }H\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
The \sphinxstyleemphasis{\_rc} in the script name is an internal abbreviation for “reconstruction”,
since we are aiming to align words from languages that are in a historical
relationship to each other. This is what happens under the hood when running
the script:

\phantomsection\label{\detokenize{mkedictor:module-ronataswestoldturkiccommands.maketoedict_rc}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.maketoedict\_rc@\spxentry{ronataswestoldturkiccommands.maketoedict\_rc}}\index{ronataswestoldturkiccommands.maketoedict\_rc@\spxentry{ronataswestoldturkiccommands.maketoedict\_rc}!module@\spxentry{module}}
\sphinxAtStartPar
Import inbuilt (collections, csv) and third party (loanpy)
functions for reading, filtering, and aligning
data. Register arguments for the command line interface.
Run the main function.
\index{register() (in module ronataswestoldturkiccommands.maketoedict\_rc)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.maketoedict\_rc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.maketoedict_rc.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.maketoedict\_rc.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register command line arguments and pass them on to the main function.
Two non\sphinxhyphen{}optional arguments will be registered:
\sphinxcode{\sphinxupquote{srclg}} (source language) and \sphinxcode{\sphinxupquote{tgtlg}} (target langauge).
Only strings contained in column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/languages.csv}} are valid
arguments.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.maketoedict\_rc)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.maketoedict\_rc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.maketoedict_rc.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.maketoedict\_rc.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read col \sphinxcode{\sphinxupquote{CV\_Segments}} in \sphinxcode{\sphinxupquote{cldf/forms.csv}}

\item {} 
\sphinxAtStartPar
Prefilter data with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.prefilter}{loanpy.utils.prefilter}

\item {} 
\sphinxAtStartPar
Apply custom alignment for historical sound changes in Uralic data
with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scminer.uralign}{uralign}.

\item {} 
\sphinxAtStartPar
Write results to \sphinxcode{\sphinxupquote{edictor/\{srclg\}2\{tgtlg\}toedict.tsv}}

\end{enumerate}

\end{fulllineitems}


\sphinxAtStartPar
Now that we have created automatic alignments for historical reconstructions,
let’s do the same for sound adaptations. This is done by running the following
command from your terminal:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.maketoedict\PYGZus{}ad\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
The \sphinxstyleemphasis{\_ad} in the script name is an internal abbreviation for “adaptation”,
since we are aiming
to align words from languages that are in a donor\sphinxhyphen{}recipient relationship to
each other. This is what happens under the hood:

\phantomsection\label{\detokenize{mkedictor:module-ronataswestoldturkiccommands.maketoedict_ad}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.maketoedict\_ad@\spxentry{ronataswestoldturkiccommands.maketoedict\_ad}}\index{ronataswestoldturkiccommands.maketoedict\_ad@\spxentry{ronataswestoldturkiccommands.maketoedict\_ad}!module@\spxentry{module}}
\sphinxAtStartPar
Import inbuilt (csv, json) and third\sphinxhyphen{}party (lingpy, loanpy) functinoalities to
read, filter, align, and write data.
Register arguments for the command line interface. Run the main function.
\index{register() (in module ronataswestoldturkiccommands.maketoedict\_ad)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.maketoedict\_ad}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.maketoedict_ad.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.maketoedict\_ad.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register command line arguments and pass them on to the main function.
Two non\sphinxhyphen{}optional arguments will be registered:
\sphinxcode{\sphinxupquote{srclg}} (source language) and \sphinxcode{\sphinxupquote{tgtlg}} (target langauge).
Only strings contained in column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/languages.csv}} are valid
arguments.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.maketoedict\_ad)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.maketoedict\_ad}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.maketoedict_ad.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.maketoedict\_ad.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read \sphinxcode{\sphinxupquote{cldf/cognates.csv}} and \sphinxcode{\sphinxupquote{cldf/forms.csv}}.

\item {} 
\sphinxAtStartPar
Loop through \sphinxcode{\sphinxupquote{cldf/cognates.csv}}.

\item {} 
\sphinxAtStartPar
Align data in column \sphinxcode{\sphinxupquote{Segments}} with \sphinxhref{https://lingpy.github.io/reference/lingpy.align.html\#lingpy.align.pairwise.Pairwise.align}{lingpy})

\item {} 
\sphinxAtStartPar
Write output file with headers
\sphinxcode{\sphinxupquote{ID}}, \sphinxcode{\sphinxupquote{COGID}}, \sphinxcode{\sphinxupquote{DOCULECT}}, \sphinxcode{\sphinxupquote{ALIGNMENT}}, \sphinxcode{\sphinxupquote{PROSODY}}.

\end{enumerate}

\end{fulllineitems}


\sphinxAtStartPar
Now that we have created suitable input files for the Edictor, it is time to
upload and edit them with our expert knowledge.


\section{Step 2: Edit horizontal and vertical sound correspondences with Edictor}
\label{\detokenize{mkedictor:step-2-edit-horizontal-and-vertical-sound-correspondences-with-edictor}}
\sphinxAtStartPar
\sphinxhref{https://digling.org/edictor/}{Edictor} is an interactive tool for managing
etymological data. We will use it to improve the automated alignments that we
have created in the previous step. To familiarise yourself with Edictor, you
can read its \sphinxhref{https://aclanthology.org/E17-3003.pdf}{release paper},
\sphinxhref{https://hcommons.org/deposits/item/hc:43687/}{this use\sphinxhyphen{}case} or watch its
\sphinxhref{https://www.youtube.com/watch?v=IyZuf6SmQM4}{Youtube tutorial}.

\sphinxAtStartPar
For editing vertical sound correspondences (i.e. historical sound changes):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Upload: Click on \sphinxcode{\sphinxupquote{Browse}}, select \sphinxcode{\sphinxupquote{edictor/H2EAHtoedict.tsv}}, click on
\sphinxcode{\sphinxupquote{Open the file}}

\item {} 
\sphinxAtStartPar
Load columns: Click on \sphinxcode{\sphinxupquote{select Columns}} on top, tick \sphinxcode{\sphinxupquote{select all}},
click \sphinxcode{\sphinxupquote{OK}}.

\item {} 
\sphinxAtStartPar
Edit alignments: Left\sphinxhyphen{}click once in the row of the \sphinxcode{\sphinxupquote{ALIGNMENT}}
column that you want to edit, edit, left\sphinxhyphen{}click again to keep the changes.

\item {} 
\sphinxAtStartPar
Syntax of alignments:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Single\sphinxhyphen{}space: separator between IPA tokens

\item {} 
\sphinxAtStartPar
Dot symbol: separator of IPA tokens within clusters of IPA\sphinxhyphen{}sounds

\item {} 
\sphinxAtStartPar
Minus symbol: gap symbol for sounds that disappeared or didn’t exist

\item {} 
\sphinxAtStartPar
Plus symbol: trimming border for prefixes and suffixes that will be
ignored in analyses. The file \sphinxcode{\sphinxupquote{etc/formspec.json}} was created
based on these.

\item {} 
\sphinxAtStartPar
Hash symbol: Word boundary

\end{itemize}

\item {} 
\sphinxAtStartPar
Cache: Click on the floppy\sphinxhyphen{}disk symbol in the top\sphinxhyphen{}right corner or use
\sphinxcode{\sphinxupquote{Ctrl + S}}

\item {} 
\sphinxAtStartPar
Download: Click on the downwards pointing arrow symbol in the top\sphinxhyphen{}right
corner or use \sphinxcode{\sphinxupquote{Ctrl + E}}. Click on \sphinxcode{\sphinxupquote{Save file}}. Move it to your local
\sphinxcode{\sphinxupquote{edictor/}} directory and name it \sphinxcode{\sphinxupquote{H2EAHedicted.tsv}}.

\end{enumerate}

\sphinxAtStartPar
In the custom\sphinxhyphen{}alignment of this use\sphinxhyphen{}case, we first clustered vowels and
consonants together with the dot\sphinxhyphen{}symbol and used spaces to separate those
clusters from each other. Wherever it seemed appropriate, we allowed sound
correspondences of one to many, e.g. in \sphinxstyleemphasis{a\textipa{:} \textless{} a.\textipa{G}.a}, which is a well studied
pattern in our data but difficult for an algorithm to catch.

\sphinxAtStartPar
For editing horizontal sound correspondences (i.e. sound adaptations):

\sphinxAtStartPar
Follow the same steps, but this time upload \sphinxcode{\sphinxupquote{edictor/WOT2EAHtoedict.tsv}}.
Here, we are allowing only one to one correspondences and ingore word
boundaries. After downloading the aligned data, a post\sphinxhyphen{}editing step is
necessary. This is carried out with following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.cvgapedicted\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
This is what happened under the hood:

\phantomsection\label{\detokenize{mkedictor:module-ronataswestoldturkiccommands.cvgapedicted}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.cvgapedicted@\spxentry{ronataswestoldturkiccommands.cvgapedicted}}\index{ronataswestoldturkiccommands.cvgapedicted@\spxentry{ronataswestoldturkiccommands.cvgapedicted}!module@\spxentry{module}}
\sphinxAtStartPar
Import inbuilt (csv) and third\sphinxhyphen{}party (loanpy) functions to read and process
data.
Register arguments for the command line interface. Run the main function.
\index{register() (in module ronataswestoldturkiccommands.cvgapedicted)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.cvgapedicted}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.cvgapedicted.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.cvgapedicted.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register command line arguments and pass them on to the main function.
Two non\sphinxhyphen{}optional arguments will be registered:
\sphinxcode{\sphinxupquote{srclg}} (source language) and \sphinxcode{\sphinxupquote{tgtlg}} (target langauge).
Only strings contained in column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/languages.csv}} are valid
arguments.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.cvgapedicted)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.cvgapedicted}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.cvgapedicted.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.cvgapedicted.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read edictor/\{srclg\}2\{tgtlg\}edicted.tsv

\item {} 
\sphinxAtStartPar
Replace “\sphinxhyphen{}” in source language data by “C” or “V” with
\sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.cvgaps}{loanpy.utils.cvgaps}.

\item {} 
\sphinxAtStartPar
Write file

\end{enumerate}

\end{fulllineitems}


\sphinxAtStartPar
This step has been outsourced to post\sphinxhyphen{}processing in order to avoid any
confusion by missing gap symbols during the manual editing process of
alignments.


\section{Step 3: Validate whether this is suitable as input for loanpy}
\label{\detokenize{mkedictor:step-3-validate-whether-this-is-suitable-as-input-for-loanpy}}
\sphinxAtStartPar
Now that we have improved the alignments by complementing the algorithmic
approach with expert knowledge, the only thing left to do is to validate
whether the format of our data is suitable as input for loanpy. This can be
done by running following two commands:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.evaledicted\PYG{+w}{ }H\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
and

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.evaledicted\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
This is what happens under the hood:

\phantomsection\label{\detokenize{mkedictor:module-ronataswestoldturkiccommands.evaledicted}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.evaledicted@\spxentry{ronataswestoldturkiccommands.evaledicted}}\index{ronataswestoldturkiccommands.evaledicted@\spxentry{ronataswestoldturkiccommands.evaledicted}!module@\spxentry{module}}
\sphinxAtStartPar
Check if the file edited by edictor is OK.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Do we have an even number of rows (excluding the header) that alternate between source and target language data (src\sphinxhyphen{}tgt\sphinxhyphen{}src\sphinxhyphen{}tgt\sphinxhyphen{}…)?

\item {} 
\sphinxAtStartPar
Do both sides of each alignment have the same number of phonemes or phoneme clusters?

\end{enumerate}

\sphinxAtStartPar
If any of these conditions is not met, an assertion error will be raised.
\index{run() (in module ronataswestoldturkiccommands.evaledicted)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.evaledicted}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkedictor:ronataswestoldturkiccommands.evaledicted.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.evaledicted.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read the manually fine\sphinxhyphen{}tuned alignments

\item {} 
\sphinxAtStartPar
Check if sequence is src\sphinxhyphen{}tgt\sphinxhyphen{}src\sphinxhyphen{}tgt\sphinxhyphen{}…
with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.is\_valid\_language\_sequence}{loanpy.utils.is\_valid\_language\_sequence}

\item {} 
\sphinxAtStartPar
Check if each word in an alignment has the same number of
phoneme (clusters) with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.is\_same\_length\_alignments}{loanpy.is\_same\_length\_alignments}

\item {} 
\sphinxAtStartPar
Logs the number of the iteration cycle to the console if there’s a
problem.

\item {} 
\sphinxAtStartPar
Logs OK if all is fine.

\end{enumerate}

\end{fulllineitems}


\sphinxAtStartPar
Both commands printing “OK” to the console means that we have successfully
edited the alignments of our etymological data set and are ready to move on
to part 3 by clicking on the \sphinxcode{\sphinxupquote{Next}} button.

\sphinxstepscope


\chapter{Part 3: Analyse data with LoanPy}
\label{\detokenize{mkloanpy:part-3-analyse-data-with-loanpy}}\label{\detokenize{mkloanpy::doc}}
\sphinxAtStartPar
The following six steps describe how to input aligned CLDF data to \sphinxhref{https://loanpy.readthedocs.io/en/latest/home.html}{loanpy}, and how to mine sound
correspondences and evaluate and visualise their predictive power.


\section{Step 1: Mine phonotactic inventory}
\label{\detokenize{mkloanpy:step-1-mine-phonotactic-inventory}}
\sphinxAtStartPar
These are necessary to predict phonotactic repairs during loanword adaptation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.mineEAHinvs\PYG{+w}{ }invs.json
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.mineEAHinvs}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.mineEAHinvs@\spxentry{ronataswestoldturkiccommands.mineEAHinvs}}\index{ronataswestoldturkiccommands.mineEAHinvs@\spxentry{ronataswestoldturkiccommands.mineEAHinvs}!module@\spxentry{module}}
\sphinxAtStartPar
Create a list of all possible prosodic structures (like “CVCV”) in the
target language and store them in a json\sphinxhyphen{}file.
\index{register() (in module ronataswestoldturkiccommands.mineEAHinvs)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.mineEAHinvs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.mineEAHinvs.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.mineEAHinvs.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments. Only one argument necessary: The name of the
output\sphinxhyphen{}file. Should end in \sphinxstyleemphasis{.json}.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.mineEAHinvs)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.mineEAHinvs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.mineEAHinvs.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.mineEAHinvs.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read the aligned data in \sphinxcode{\sphinxupquote{edictor/WOT2EAHedicted.tsv}} with the
inbuilt csv package

\item {} 
\sphinxAtStartPar
Pass it on to loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scminer.get\_prosodic\_inventory}{get\_prosodic\_inventory}
function, which will extract all phonotactic structures (like “CVCV”)
from the target language.

\item {} 
\sphinxAtStartPar
Write the inventory of prosodic structures to a json\sphinxhyphen{}file with the
inbuilt json package. It will have the name that was passed on as an
argument to the command and will be written to the folder \sphinxcode{\sphinxupquote{loanpy}}.

\end{enumerate}

\end{fulllineitems}



\section{Step 2: Create heuristic sound substitutions}
\label{\detokenize{mkloanpy:step-2-create-heuristic-sound-substitutions}}
\sphinxAtStartPar
Since any existing phoneme can be adapted when entering a language through
a loanword, we have to create a heuristic adaptation prediction for as many
IPA characters as possible, in this case 6491.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.makeheur\PYG{+w}{ }EAH\PYG{+w}{ }heur.json
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.makeheur}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.makeheur@\spxentry{ronataswestoldturkiccommands.makeheur}}\index{ronataswestoldturkiccommands.makeheur@\spxentry{ronataswestoldturkiccommands.makeheur}!module@\spxentry{module}}
\sphinxAtStartPar
Create heuristic predictions of phoneme adaptations based on feature vector
similarities of phonemes.
\index{register() (in module ronataswestoldturkiccommands.makeheur)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.makeheur}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.makeheur.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.makeheur.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments. Two argument necessary: The ID of the target language,
i.e. the one in which loanwords are adapted. Valid IDs can be found in
column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/language.csv}}. The second argument is the name of
the output\sphinxhyphen{}file. Should end in \sphinxstyleemphasis{.json}.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.makeheur)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.makeheur}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.makeheur.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.makeheur.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Pass on the target language ID as defined in \sphinxcode{\sphinxupquote{etc/languages.csv}}
to loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scminer.get\_heur}{get\_heur}
function,
which will read the \sphinxcode{\sphinxupquote{cldf/.transcription\sphinxhyphen{}report.json}} file and
extract the phoneme inventory of the target language from there.
It will also read the file \sphinxcode{\sphinxupquote{ipa\_all.csv}}, which is shipped with
loanpy. From these two files it creates a heuristic prediction of
phoneme substitution patterns in loanword adaptations.

\item {} 
\sphinxAtStartPar
Write the results to a file named according to the value passed to the
second argument. It will be written to the folder \sphinxcode{\sphinxupquote{loanpy}}.
Expected file size: ca. 2.5MB.

\end{enumerate}

\end{fulllineitems}



\section{Step 3: Mine vertical and horizontal sound correspondences}
\label{\detokenize{mkloanpy:step-3-mine-vertical-and-horizontal-sound-correspondences}}
\sphinxAtStartPar
The output will serve as fuel for predicting loanword adaptations and
historical reconstructions later on.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.minesc\PYG{+w}{ }H\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.minesc\PYG{+w}{ }WOT\PYG{+w}{ }EAH\PYG{+w}{ }heur.json
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.minesc}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.minesc@\spxentry{ronataswestoldturkiccommands.minesc}}\index{ronataswestoldturkiccommands.minesc@\spxentry{ronataswestoldturkiccommands.minesc}!module@\spxentry{module}}
\sphinxAtStartPar
Read in aligned data and write a json\sphinxhyphen{}file with information about sound
correspondences.
\index{register() (in module ronataswestoldturkiccommands.minesc)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.minesc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.minesc.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.minesc.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments. Two arguments necessary: The ID of the target and
source language: In horizontal transfers, the donor language is the source
and the recipient language is the target. In vertical transfers, the
target language is the ancestor and the source the descendant for backwards
reconstructions. Valid IDs can be found in
column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/language.csv}}. A third argument is optional,
namely the path to the json\sphinxhyphen{}file containing the heuristic phoneme
adaptations.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.minesc)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.minesc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.minesc.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.minesc.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
If argument three was provided, read the json\sphinxhyphen{}file containing heuristic
phoneme adaptation predictions with the inbuilt json package.

\item {} 
\sphinxAtStartPar
Read aligned forms from \sphinxcode{\sphinxupquote{edictor/\{srclg\}2\{tgtlg\}edicted.tsv}}

\item {} 
\sphinxAtStartPar
Extract sound and phonotactic correspondences from the data with
loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.scminer.get\_correspondences}{get\_corrspondences}
function

\item {} 
\sphinxAtStartPar
Write the sound and phonotactic correspondences to a file named
\sphinxcode{\sphinxupquote{\{srclg\}2\{tgtlg\}sc.json}} in the folder \sphinxcode{\sphinxupquote{loanpy}}.

\end{enumerate}

\end{fulllineitems}



\section{Step 4: Make sound correspondences human\sphinxhyphen{}readable}
\label{\detokenize{mkloanpy:step-4-make-sound-correspondences-human-readable}}
\sphinxAtStartPar
The sound\sphinxhyphen{}correspondence file is stored as a computer\sphinxhyphen{}readable json.
To create a human\sphinxhyphen{}readable tsv\sphinxhyphen{}file, run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.vizsc\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.vizsc\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.vizsc}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.vizsc@\spxentry{ronataswestoldturkiccommands.vizsc}}\index{ronataswestoldturkiccommands.vizsc@\spxentry{ronataswestoldturkiccommands.vizsc}!module@\spxentry{module}}
\sphinxAtStartPar
Read json file containing six dictionaries about sound and phonotactic
correspondences and turn it into a human\sphinxhyphen{}readable tsv\sphinxhyphen{}file with additional
info for easier manual inspection.
\index{register() (in module ronataswestoldturkiccommands.vizsc)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.vizsc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.vizsc.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.vizsc.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments. Two arguments necessary: The ID of the target and
source language: In horizontal transfers, the donor language is the source
and the recipient language is the target. In vertical transfers, the
target language is the ancestor and the source the descendant for backwards
reconstructions. Valid IDs can be found in
column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/language.csv}}.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.vizsc)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.vizsc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.vizsc.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.vizsc.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read sound\sphinxhyphen{}correspondence json at \sphinxcode{\sphinxupquote{loanpy/\{srclg\}2\{tgtlg\}sc.json}}.

\item {} 
\sphinxAtStartPar
Transform computer\sphinxhyphen{}readable data structure to human\sphinxhyphen{}readable tables
with \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.utils.scjson2tsv}{loanpy.utils.scjson2tsv}

\item {} 
\sphinxAtStartPar
Merge IDs with info from related tables for easier manual inspection.

\item {} 
\sphinxAtStartPar
Write the correspondence tables to two files named
\sphinxcode{\sphinxupquote{\{srclg\}2\{tgtlg\}sc.tsv}} and \sphinxcode{\sphinxupquote{\{srclg\}2\{tgtlg\}sc\_phonotactics.tsv}}
in the folder \sphinxcode{\sphinxupquote{loanpy}}.

\end{enumerate}

\end{fulllineitems}



\section{Step 5: Evaluate vertical and horizontal sound correspondences}
\label{\detokenize{mkloanpy:step-5-evaluate-vertical-and-horizontal-sound-correspondences}}
\sphinxAtStartPar
In this section, we are checking the predictive power of the mined
sound correspondences with loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.eval\_sca.eval\_all}{eval\_all}
function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.evalsc\PYG{+w}{ }H\PYG{+w}{ }EAH\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}[10, 100, 500, 700, 1000, 5000, 7000]\PYGZdq{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.evalsc\PYG{+w}{ }WOT\PYG{+w}{ }EAH\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}[10, 100, 500, 700, 1000, 5000, 7000]\PYGZdq{}}\PYG{+w}{ }True\PYG{+w}{ }True\PYG{+w}{ }heur.json
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.evalsc}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.evalsc@\spxentry{ronataswestoldturkiccommands.evalsc}}\index{ronataswestoldturkiccommands.evalsc@\spxentry{ronataswestoldturkiccommands.evalsc}!module@\spxentry{module}}
\sphinxAtStartPar
Specify the aligned wordlist data together with a few arguemnts
and pass them on to loanpy’s evaluator module. Write the output
to a json\sphinxhyphen{}file. It is a list of tuples with pairs of
false and true positives.
\index{register() (in module ronataswestoldturkiccommands.evalsc)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.evalsc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.evalsc.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.evalsc.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register arguments. Three arguments necessary: The first two are the IDs
of the target and source language, as specified in column \sphinxcode{\sphinxupquote{ID}} in
\sphinxcode{\sphinxupquote{etc/language.csv}}. The third is a list of integers that specifies
how many guesses should be made per input word during each iteration.
This roughly corresponds to the false positive rate. Another three
arguments are optional: Set adapt=True if we are dealing with horizontal
transfers, prosody=True if phonotactics should be repaired during
loanword adaptation prediction, and pass a file name to parameter “heur”,
to add heuristic predictions of sound adaptations.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.evalsc)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.evalsc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.evalsc.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.evalsc.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read aligned data between forms of the source and target language
with the inbuilt csv library.

\item {} 
\sphinxAtStartPar
If a filename was passed to parameter “heur”, open that file. It has
to be located in the \sphinxcode{\sphinxupquote{loanpy}} folder.

\item {} 
\sphinxAtStartPar
Read the other parameters with the inbuilt \sphinxcode{\sphinxupquote{literal\_eval}} function.

\item {} 
\sphinxAtStartPar
Pass the parameters and the data to loanpy’s \sphinxhref{https://loanpy.readthedocs.io/en/latest/documentation.html\#loanpy.eval\_sca.eval\_all}{eval\_all}
function

\item {} 
\sphinxAtStartPar
Write the results to a file called \sphinxcode{\sphinxupquote{tpfp\{srclg\}2\{tgtlg\}.json}}.

\end{enumerate}

\end{fulllineitems}



\section{Step 6: Plot the evaluations}
\label{\detokenize{mkloanpy:step-6-plot-the-evaluations}}
\sphinxAtStartPar
To gauge the performance of the model, we can plot an \sphinxhref{https://en.wikipedia.org/wiki/Receiver\_operating\_characteristic}{ROC} curve,
calculate its optimum cut\sphinxhyphen{}off value and its area under the curve \sphinxhref{https://en.wikipedia.org/wiki/Receiver\_operating\_characteristic\#Area\_under\_the\_curve}{(AUC)},
a common metric to evaluate predictive models:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cldfbench\PYG{+w}{ }ronataswestoldturkic.plot\PYGZus{}eval\PYG{+w}{ }H\PYG{+w}{ }EAH
cldfbench\PYG{+w}{ }ronataswestoldturkic.plot\PYGZus{}eval\PYG{+w}{ }WOT\PYG{+w}{ }EAH
\end{sphinxVerbatim}

\sphinxAtStartPar
The results:

\sphinxAtStartPar
Predicting reconstructions from modern Hungarian words:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{H2EAH}.jpeg}
\caption{The ROC curve shows how the relative number of true positives (y\sphinxhyphen{}axis)
increases, as the relative number of false positives (x\sphinxhyphen{}axis) increases.
The optimal cut\sphinxhyphen{}off point is at 700 false positives per word, which yields
284 correct reconstructions out of 406 (i.e. 70\%). The AUC is just above
0.7, which \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1556086415306043}{is considered acceptable}.
Note that the relative number of false positives and the AUC stay the same,
irrespective of whether false positives are counted on a
per\sphinxhyphen{}word basis (7000) or as an aggregate sum (7000 * 813= 5,691,000).
The absolute number of possible true positives (406) was reached after
prefiltering the 512 cognate sets of the raw data in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkedictor.html\#ronataswestoldturkiccommands.maketoedict\_rc.run}{Part 2 step 1}.}\label{\detokenize{mkloanpy:id2}}\end{figure}

\sphinxAtStartPar
The performance of this model can be improved by removing irregular sound
correspondences. By inspecting the file \sphinxtitleref{loanpy/H2EAHsc.tsv} we can see that
many words contain sound correspondences that occur only once throughout
the entire etymological dictionary. Counting the number of those cognate sets
shows that 106 out of 406 or 26\% of all etymologies contain at least one sound
correspondence that is irregular, i.e. occurs only in one single etymology.
(Note that the pre\sphinxhyphen{}filtering did not skew this ratio because it picked all
cognate sets with an Early Ancient Hungarian and a Hungarian counterpart.)
If we remove those 106 cognate sets with irregular sound correspondences
from our training and test data, 300 cognate sets remain and we get following
result:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{H2EAH1}.jpeg}
\caption{This model performs significantly better than the previous one. At its
optimum of 100 guesses per word it reconstructs 279 out of 300 forms (93\%)
correctly. The
AUC is above 0.9, which \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1556086415306043}{is considered outstanding}.}\label{\detokenize{mkloanpy:id3}}\end{figure}

\sphinxAtStartPar
Predicting loanword adaptations from West Old Turkic words:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{WOT2EAH}.jpeg}
\caption{Out of 512 etymologies, 384 contained loanword adaptations from West
Old Turkic into Early ancient Hungarian. This pre\sphinxhyphen{}filtering was carried
out in \sphinxhref{https://ronataswestoldturkic.readthedocs.io/en/latest/mkedictor.html\#ronataswestoldturkiccommands.maketoedict\_rc.run}{Part 2 step 1}.
At its optimum of 100 guesses per word, the model predicted 346 words
correctly out of 384 (90\%). The
AUC is above 0.9, which \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1556086415306043}{is considered outstanding}.}\label{\detokenize{mkloanpy:id4}}\end{figure}

\sphinxAtStartPar
What happened under the hood:

\phantomsection\label{\detokenize{mkloanpy:module-ronataswestoldturkiccommands.plot_eval}}\index{module@\spxentry{module}!ronataswestoldturkiccommands.plot\_eval@\spxentry{ronataswestoldturkiccommands.plot\_eval}}\index{ronataswestoldturkiccommands.plot\_eval@\spxentry{ronataswestoldturkiccommands.plot\_eval}!module@\spxentry{module}}
\sphinxAtStartPar
Plot the results of the sound correrspondence file evaluation.
\index{auc() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{auc()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.auc}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{auc}}}{\sphinxparam{\DUrole{n,n}{points}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ float}}
\pysigstopsignatures
\sphinxAtStartPar
Calculate the area under the curve with the \sphinxhref{https://en.wikipedia.org/wiki/Trapezoidal\_rule}{trapezoidal rule}
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{points}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{tuples}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{integers}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{floats}}) \textendash{} A list of x\sphinxhyphen{}y\sphinxhyphen{}coordinates

\sphinxlineitem{Returns}
\sphinxAtStartPar
The area under the curve

\sphinxlineitem{Return type}
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{euclidean\_distance() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{euclidean\_distance()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.euclidean_distance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{euclidean\_distance}}}{\sphinxparam{\DUrole{n,n}{point1}}, \sphinxparam{\DUrole{n,n}{point2}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Calculate the \sphinxhref{https://en.wikipedia.org/wiki/Euclidean\_distance}{Euclidean distance} between two points.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{point1}} (\sphinxstyleliteralemphasis{\sphinxupquote{a tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{two integers}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{floats}}) \textendash{} The first point

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{point2}} (\sphinxstyleliteralemphasis{\sphinxupquote{a tuple}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{two integers}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{floats}}) \textendash{} The second point

\end{itemize}

\sphinxlineitem{Returns}
\sphinxAtStartPar
The euclidean distance

\sphinxlineitem{Return type}
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{find\_optimum() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{find\_optimum()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.find_optimum}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{find\_optimum}}}{\sphinxparam{\DUrole{n,n}{points}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ Tuple\DUrole{p,p}{{[}}int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Calculate the euclidean distance of each point to the upper left hand
corner and return the point with the lowest distance
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{points}} (\sphinxstyleliteralemphasis{\sphinxupquote{a list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{tuples}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{floats}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{integers}}) \textendash{} A list of coordinates representing points in the graph.

\sphinxlineitem{Returns}
\sphinxAtStartPar
The optimal cut\sphinxhyphen{}off point of the ROC curve

\sphinxlineitem{Return type}
\sphinxAtStartPar
a tuple of two floats or integers

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_curve() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{plot\_curve()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.plot_curve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{plot\_curve}}}{\sphinxparam{\DUrole{n,n}{points}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }float\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}, \sphinxparam{\DUrole{n,n}{absfp}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}int\DUrole{p,p}{{]}}}}, \sphinxparam{\DUrole{n,n}{maxtp}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}, \sphinxparam{\DUrole{n,n}{file\_name}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{str\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }Path}}}{{ $\rightarrow$ None}}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Get a list of x\sphinxhyphen{} and y\sphinxhyphen{}axis values out of the points argument

\item {} 
\sphinxAtStartPar
Plot them to a graph with matplotlib, add lables to axes.

\item {} 
\sphinxAtStartPar
Calculate the area under the curve and add it to the plot title

\item {} 
\sphinxAtStartPar
Calculate the optimal cut\sphinxhyphen{}off point and mark it with an “x” on the
graph.

\item {} 
\sphinxAtStartPar
Add a legend to the plot and write the image as a jpeg to the specified
path

\end{enumerate}
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{points}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{tuples}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{floats}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{integers}}) \textendash{} List of coordinate points as tuples

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{absfp}} (\sphinxstyleliteralemphasis{\sphinxupquote{a list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{integers}}) \textendash{} The absolute numbers of guesses made. Needed to add
information to legend on how many guesses are the optimum.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{maxtp}} (\sphinxstyleliteralemphasis{\sphinxupquote{an integer}}) \textendash{} The absolute number of possible true positives. Needed to
contextualise the relative information on the plot as a
human reader.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{a string}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{pathlike object}}) \textendash{} The desired name and location of the output jpeg\sphinxhyphen{}file.

\end{itemize}

\sphinxlineitem{Returns}
\sphinxAtStartPar
Writes the images to the specified path, returns None

\sphinxlineitem{Return type}
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{register() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{register()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.register}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{register}}}{\sphinxparam{\DUrole{n,n}{parser}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Register command line arguments and pass them on to the main function.
Two non\sphinxhyphen{}optional arguments will be registered:
\sphinxcode{\sphinxupquote{srclg}} (source language) and \sphinxcode{\sphinxupquote{tgtlg}} (target langauge).
Only strings contained in column \sphinxcode{\sphinxupquote{ID}} in \sphinxcode{\sphinxupquote{etc/languages.csv}} are valid
arguments.

\end{fulllineitems}

\index{run() (in module ronataswestoldturkiccommands.plot\_eval)@\spxentry{run()}\spxextra{in module ronataswestoldturkiccommands.plot\_eval}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mkloanpy:ronataswestoldturkiccommands.plot_eval.run}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{ronataswestoldturkiccommands.plot\_eval.}}\sphinxbfcode{\sphinxupquote{run}}}{\sphinxparam{\DUrole{n,n}{args}}}{}
\pysigstopsignatures\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Read file \sphinxcode{\sphinxupquote{loanpy/tpfp\{srclg\}2\{tgtlg\}.json}} containing true\sphinxhyphen{}positive
false\sphinxhyphen{}positive ratios, the length of the dataframe with header
(“maxtp”) and the guesslist, generated by the \sphinxcode{\sphinxupquote{evalsc}} command.

\item {} 
\sphinxAtStartPar
Plot the data to an ROC\sphinxhyphen{}curve, providing the AUC and optimal cut\sphinxhyphen{}off.

\end{enumerate}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{r}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.\_\_init\_\_}\sphinxstyleindexpageref{TL;DR:\detokenize{module-ronataswestoldturkiccommands.__init__}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.cvgapedicted}\sphinxstyleindexpageref{mkedictor:\detokenize{module-ronataswestoldturkiccommands.cvgapedicted}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.evaledicted}\sphinxstyleindexpageref{mkedictor:\detokenize{module-ronataswestoldturkiccommands.evaledicted}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.evalsc}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.evalsc}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.makeheur}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.makeheur}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.maketoedict\_ad}\sphinxstyleindexpageref{mkedictor:\detokenize{module-ronataswestoldturkiccommands.maketoedict_ad}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.maketoedict\_rc}\sphinxstyleindexpageref{mkedictor:\detokenize{module-ronataswestoldturkiccommands.maketoedict_rc}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.mineEAHinvs}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.mineEAHinvs}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.minesc}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.minesc}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.plot\_eval}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.plot_eval}}
\item\relax\sphinxstyleindexentry{ronataswestoldturkiccommands.vizsc}\sphinxstyleindexpageref{mkloanpy:\detokenize{module-ronataswestoldturkiccommands.vizsc}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}